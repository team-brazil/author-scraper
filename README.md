# Guia comentado do script OpenAlex (Economia)

Este documento explica **em detalhes, passo a passo**, o que cada parte do seu script faz. O objetivo do
script √© **coletar autores relevantes na √°rea de Economia** a partir da API p√∫blica do **OpenAlex**, aplicar
**crit√©rios estritos** de filtragem por relev√¢ncia de conceitos, e **salvar os resultados** em um arquivo CSV, com
possibilidade de **retomada** (checkpoint via cursor) e **parada graciosa** (Ctrl+C).

## Sum√°rio

[1. Vis√£o geral do que o script faz](#1-vis√£o-geral-do-que-o-script-faz)

[2. Pr√©-requisitos e como executar](#2-pr√©-requisitos-e-como-executar)

[3. Cabe√ßalho e codifica√ß√£o](#3-cabe√ßalho-e-codifica√ß√£o)

[4. Imports ‚Äî bibliotecas usadas](#4-imports--bibliotecas-usadas)

[5. Configura√ß√µes principais](#5-configura√ß√µes-principais)

[6. Pacing adaptativo e timeouts](#6-pacing-adaptativo-e-timeouts)

[7. Filtros "estritos por√©m inclusivos"](#7-filtros-estritos-por√©m-inclusivos)

[8. Sess√£o HTTP e cabe√ßalhos](#8-sess√£o-http-e-cabe√ßalhos)

[9. Esquema do CSV (colunas)](#9-esquema-do-csv-colunas)

[10. Helpers utilit√°rios](#10-helpers-utilit√°rios)
  - `_cid`
  - `parse_retry_after`
  - `_get`
    
[11. Pr√©-carregamento dos descendentes de Economia](#11-pr√©-carregamento-dos-descendentes-de-economia)

[12. Contagens de trabalhos (para borderline)](#12-contagens-de-trabalhos-para-borderline)

[13. Filtro principal do autor: l√≥gica completa](#13-filtro-principal-do-autor-l√≥gica-completa)

[14. Utilidades de CSV e cursor](#14-utilidades-de-csv-e-cursor)

[15. Parada graciosa (Ctrl+C)](#15-parada-graciosa-ctrlc)

[16. Loop principal de coleta (fetch_economics_authors)](#16-loop-principal-de-coleta--fetch_economics_authors-)

[17. Ponto de entrada `if __name__ == "__main__"`](#17-ponto-de-entrada)

[18. Boas pr√°ticas embutidas no desenho](#18-boas-pr√°ticas-embutidas-no-desenho)

[19. Ajustes e personaliza√ß√µes comuns](#19-ajustes-e-personaliza√ß√µes-comuns)

[20. Erros comuns e como lidar](#20-erros-comuns-e-como-lidar)

## 1. Vis√£o geral do que o script faz

- Consulta a API do OpenAlex em `/authors` para listar autores ligados ao conceito Economia (ID `C162324750` ).
- Pr√©-carrega todos os subconceitos de Economia (sub√°rvore) para avaliar relev√¢ncia sem precisar de chamadas extras por autor.
- Aplica um filtro estrito baseado em score absoluto, posi√ß√£o em top-K, for√ßa relativa e (quando necess√°rio) propor√ß√£o de trabalhos do autor em Economia.
- Grava os autores aprovados no CSV com campos padronizados.
- Gerencia limites de taxa (429) com backoff e ajusta ‚Äúsleep‚Äù automaticamente.
- Permite retomar o processo de onde parou via cursor salvo em arquivo.
- Suporta parada graciosa com Ctrl+C (SIGINT), finalizando a p√°gina corrente e salvando checkpoint.

## 2. Pr√©-requisitos e como executar

Requisitos

- Python 3.8+ (recomendado).
- Biblioteca requests instalada.

```
  pip install requests
```

Execu√ß√£o

Salve o script em um arquivo, por exemplo openalex_econ.py , e execute:

```
python openalex_econ.py
```

Os resultados ser√£o gravados em `openalex_field_outputs/economics_researchers_strict.csv` e o cursor em `openalex_field_outputs/economics_cursor.txt`.

## 3. Cabe√ßalho e codifica√ß√£o

```python
  #!/usr/bin/env python
  # coding: utf-8
```

- Shebang: usa o Python presente no PATH do ambiente (port√°vel entre sistemas). - Codifica√ß√£o: define UTF-8 para permitir acentos, emojis e caracteres especiais.

## 4. Imports ‚Äî bibliotecas usadas

```python
  import os, re, csv, time, signal, requests
  from datetime import datetime, timezone
  from email.utils import parsedate_to_datetime
  from functools import lru_cache
```

- `os` : arquivos, diret√≥rios, caminhos.
- `re` : express√µes regulares (sanitiza√ß√£o de strings).
- `csv` : escrita segura de CSV (com cabe√ßalho e escaping corretos).
- `time` : sleep e tempo simples.
- `signal` : captura sinais do sistema (Ctrl+C) para parar com seguran√ßa.
- `requests` : HTTP para consumir a API do OpenAlex.
- `datetime` , `timezone` : datas/horas conscientes de fuso.
- `parsedate_to_datetime` : interpreta `Retry-After` no formato de data HTTP. - `lru_cache` : memoiza√ß√£o de fun√ß√µes de contagem (efici√™ncia).

## 5. Configura√ß√µes principais

```python
  ECONOMICS_ID = "C162324750"
  ECONOMICS_NAME = "Economics"
```

- Conceito **raiz** (Economia) e seu **nome** leg√≠vel.

```python
  OUTPUT_DIR = "openalex_field_outputs"
  os.makedirs(OUTPUT_DIR, exist_ok=True)
```

- Pasta de sa√≠da; criada se n√£o existir.

```python
  SAFE_FIELD = re.sub(r'[^a-z0-9_-]', '', ECONOMICS_NAME.strip().replace(' ',
  '_').lower())
```

- Gera um nome de arquivo seguro (min√∫sculas, `_` no lugar de espa√ßo, e remo√ß√£o de caracteres especiais).

```python
  OUT_PATH = os.path.join(OUTPUT_DIR, f"{SAFE_FIELD}_researchers_strict.csv")
  CURSOR_PATH = os.path.join(OUTPUT_DIR, f"{SAFE_FIELD}_cursor.txt")
```

- Caminho do arquivo CSV final e do cursor para retomada.

```python
  PER_PAGE_AUTHORS = 200
```

- Tamanho de p√°gina para `/authors` (mais alto = menos idas/voltas).

## 6. Pacing adaptativo e timeouts

```python
  SLEEP = 0.15
  MIN_SLEEP, MAX_SLEEP = 0.05, 1.25
  BACKOFF_MULT, COOLDOWN_MULT = 1.5, 0.9
```

- `SLEEP` : pausa padr√£o entre p√°ginas.
- `MIN_SLEEP` / `MAX_SLEEP` : limites inferior/superior para o sono adaptativo.
- `BACKOFF_MULT` : aumenta o ‚Äúsleep‚Äù em casos de erro/limite de taxa.
- `COOLDOWN_MULT` : reduz gradualmente o ‚Äúsleep‚Äù quando as respostas est√£o est√°veis.

```
  AUTHORS_TIMEOUT = 20
  CONCEPTS_TIMEOUT = 20
  WORKS_TIMEOUT = 25
```

- Tempo m√°ximo (segundos) para aguardar cada tipo de requisi√ß√£o.

## 7. Filtros ‚Äúestritos por√©m inclusivos‚Äù

```python
  MIN_ECON_SCORE = 20                  # m√≠nimo absoluto (0‚Äì100)
  REQUIRE_ECON_TOP_K = 5               # Economia precisa estar no top-5 conceitos
  MIN_ECON_RELATIVE = 0.6              # score de Economia >= 60% do score do conceito
  top
  BORDERLINE_SCORE = 45                # abaixo disso, exige checar propor√ß√£o de
  trabalhos
  MIN_ECON_SHARE = 0.40                # se borderline: ‚â•40% dos trabalhos devem ser de
  Economia
  SLEEP_BETWEEN_COUNTS = 0.1           # pausa entre consultas de contagem
```

- **Score m√≠nimo** absoluto protege contra ‚Äúru√≠do‚Äù.
- **Top-K** garante que Economia aparece entre os principais temas do autor.
- **For√ßa relativa** impede casos em que o autor √© majoritariamente de outra √°rea.
- **Borderline**: quando o score de Economia n√£o √© alto, a **propor√ß√£o de trabalhos** em Economia precisa confirmar a relev√¢ncia.

```python
  SKIP_SHARE_IF_TOP_IS_ECON = True
```

- Se o **conceito principal** do autor j√° for de Economia, **pula** a checagem de propor√ß√£o (economia de chamadas).

## 8. Sess√£o HTTP e cabe√ßalhos

```python
  SESSION = requests.Session()
  SESSION.headers.update({"Accept-Encoding": "gzip", "User-Agent": "econ-fast/
  2.0"})
```

- Reutiliza conex√µes (HTTP keep-alive).
- Pede compress√£o gzip (menos banda).
- Define um `User-Agent` identific√°vel (boa pr√°tica com APIs p√∫blicas).

## 9. Esquema do CSV (colunas)

```python
  CSV_FIELDS = [
    "author_id","name","orcid",
    "institution_id","affiliation","country",
    "works_count","cited_by_count",
    "fields","field_group",
    "primary_concept_id","primary_concept_name","primary_concept_score",
    "best_in_field_score","best_in_field_id","best_in_field_name",
    "is_primary_in_field"
  ]
```

- Define a **ordem** e o **conjunto** de campos persistidos no CSV.

## 10. Helpers utilit√°rios

```python
_cid(s: str) -> str
```

Extrai o ID do final de uma URL (ex.: `https://openalex.org/C123` ‚Üí `C123` ).

```python
parse_retry_after(h, default_seconds=2)
```

- Interpreta o cabe√ßalho `Retry-After` da API.
- Se for um **inteiro**, usa como segundos.
- Se for uma **data HTTP**, calcula a diferen√ßa para ‚Äúagora‚Äù.
- Se falhar, retorna `default_seconds` .
- Garante retorno **‚â• 1 segundo** quando em formato data.

```python
_get(url, params=None, timeout=30)
```

Wrapper simples para `SESSION.get` com `params` e `timeout` apropriados.

## 11. Pr√©-carregamento dos descendentes de Economia

```python
  def load_econ_descendants():
      base = "https://api.openalex.org/concepts"
      cursor = "*"
      ids = {ECONOMICS_ID}
       sleep_s = 0.2
       print("üîé Preloading Economics subtree (concept IDs)...")
       while True:
           r = _get(base, params={
               "filter": f"ancestors.id:{ECONOMICS_ID}",
               "per-page": 200,
               "cursor": cursor,
               "select": "id"
           }, timeout=CONCEPTS_TIMEOUT)
           ...
```

- Percorre todas as p√°ginas de `/concepts` com filtro `ancestors.id:<ECON_ID>` para obter **toda a sub√°rvore** de Economia.
- Solicita somente o campo `id` para resposta m√≠nima.
- Trata 429 (limite de taxa) respeitando `Retry-After` e faz **backoff**.
- **Trata 5xx** com backoff exponencial suave.
- Em sucesso, aplica **cooldown** (reduz levemente o `sleep` ).
- Atualiza `cursor` e pausa entre p√°ginas para polidez.
- Retorna um `set` com todos os IDs (inclui o `ECONOMICS_ID` ).

  Benef√≠cio: depois disso, verificar se um conceito do autor pertence a Economia √© O(1) (consulta a um set ), sem chamadas extras por autor.

## 12. Contagens de trabalhos (para borderline)

```python
_count_works(filter_str: str) -> int
```

- Chama `/works` com `per-page=1` e `select=id` apenas para ler `meta.count` (n√∫mero total de itens que satisfazem o filtro).
- Retorna 0 em caso de falha.

```python
  _author_total_works(author_id_url: str) -> int #memoizado
```

- Conta todos os trabalhos do autor via filtro `authorships.author.id:<AID>` .
- **Pausa curta** ( `SLEEP_BETWEEN_COUNTS` ) por cortesia.
- **Memoizado** por `lru_cache` para n√£o repetir chamadas iguais.

```python
  _author_econ_works(author_id_url: str, econ_id: str) -> int #memoizado
```

- Conta trabalhos do autor **que s√£o de Economia** via filtro `concepts.id:<ECON_ID>` .
- Idem pausa e memoiza√ß√£o.

```python
  econ_share_ok(author_id_url, econ_id, min_share) -> bool
```

- Calcula propor√ß√£o `econ / total` e compara com `min_share` (ex.: 0.40).
- Retorna `False` se `total <= 0` .

  Essas contagens **s√≥ s√£o usadas** quando um autor est√° em **zona borderline** (score de Economia < `BORDERLINE_SCORE` ) ‚Äî reduzindo o custo total de API.

## 13. Filtro principal do autor: l√≥gica completa

```python
def author_passes_field_filter_strict(author: dict, econ_desc: set):
    xcs = author.get("x_concepts") or []
    if not xcs:
        return False, {}

    # Normaliza e ordena conceitos por score decrescente
    concepts = [{
        "id": _cid(c.get("id")),
        "display_name": c.get("display_name"),
        "score": float(c.get("score") or 0.0)
    } for c in xcs]
    concepts.sort(key=lambda z: z["score"], reverse=True)

    top = concepts[0]

    # 1) Economia aparece no top-K?
    if REQUIRE_ECON_TOP_K and REQUIRE_ECON_TOP_K > 0:
        if not any(c["id"] in econ_desc for c in concepts[:REQUIRE_ECON_TOP_K]):
            return False, {}

    # 2) Melhor conceito de Economia com score m√≠nimo
    best_econ = None
    best_econ_score = 0.0
    for c in concepts:
        if c["id"] in econ_desc and c["score"] >= float(MIN_ECON_SCORE):
            if c["score"] > best_econ_score:
                best_econ = c
                best_econ_score = c["score"]
    if best_econ is None:
        return False, {}

    # 3) For√ßa relativa: Economia forte o bastante vs. conceito top?




      if MIN_ECON_RELATIVE is not None:
          if best_econ_score < MIN_ECON_RELATIVE * float(top["score"] or 0.0):
              return False, {}

      # 4) Se borderline, verificar participa√ß√£o de trabalhos em Economia
      if best_econ_score < BORDERLINE_SCORE:
          if SKIP_SHARE_IF_TOP_IS_ECON and (top["id"] in econ_desc):
              pass
          else:
             if not econ_share_ok(author.get("id"), ECONOMICS_ID,
 MIN_ECON_SHARE):
                  return False, {}

      details = {
          "primary_concept_id": top["id"],
          "primary_concept_name": top["display_name"],
          "primary_concept_score": top["score"],
          "best_in_field_score": best_econ_score,
          "best_in_field_id": best_econ.get("id"),
          "best_in_field_name": best_econ.get("display_name"),
          "is_primary_in_field": top["id"] in econ_desc
      }
      return True, details
```

##### Resumo da l√≥gica:

1. **Top-K**: Economia (ou sub) precisa estar entre os K conceitos mais fortes do autor.
2. **Score m√≠nimo** absoluto para o melhor conceito de Economia.
3. **For√ßa relativa**: o melhor conceito de Economia deve ser ‚â• 60% do score do conceito top do autor.
4. **Borderline**: se o score de Economia n√£o atinge `BORDERLINE_SCORE` , exige **‚â• 40%** dos trabalhos em Economia, a menos que o conceito top j√° seja de Economia.
5. Se aprovado, retorna `True` e um dicion√°rio de **detalhes** para preencher o CSV.

## 14. Utilidades de CSV e cursor

```python
  def init_csv(path):
      write_header = not os.path.exists(path) or os.path.getsize(path) == 0
       f = open(path, "a", newline="", encoding="utf-8")
       w = csv.DictWriter(f, fieldnames=CSV_FIELDS)
       if write_header:
           w.writeheader()
       return f, w
```

- Abre o CSV em **append** e escreve o **cabe√ßalho** se necess√°rio.

```python
  def save_cursor(next_cursor: str | None):
      if next_cursor:
          with open(CURSOR_PATH, "w", encoding="utf-8") as fh:
              fh.write(next_cursor)
```

- Persiste o **cursor** da p√°gina seguinte (checkpoint).

```python
  def load_cursor():
      if os.path.exists(CURSOR_PATH):
          s = open(CURSOR_PATH, "r", encoding="utf-8").read().strip()
          if s:
              return s
      return "*"
```

- L√™ o cursor salvo; se n√£o houver, retorna `"*"` (in√≠cio da pagina√ß√£o).

## 15. Parada graciosa (Ctrl+C)

```python
  _SHOULD_STOP = False

  def _handle_sigint(signum, frame):
      global _SHOULD_STOP
      _SHOULD_STOP = True
      print("\nüõë Interrupt received ‚Äî finishing current page and
  checkpointing...")

  signal.signal(signal.SIGINT, _handle_sigint)
```

- Ao receber **SIGINT** (Ctrl+C), define `_SHOULD_STOP=True` .
- O loop principal verifica a flag e **para no fim da p√°gina**, salvando o cursor e fechando o CSV.

## 16. Loop principal de coleta ( `fetch_economics_authors` )

```python
  def fetch_economics_authors():
      econ_desc = load_econ_descendants()   # prefetch
      base_url = "https://api.openalex.org/authors"
      cursor = load_cursor()
      scanned = kept_total = 0




    total_candidates = None
    sleep_s = SLEEP

    print(f"üì• Starting: {ECONOMICS_NAME} (min_score={MIN_ECON_SCORE},
top_k={REQUIRE_ECON_TOP_K}, rel‚â•{MIN_ECON_RELATIVE},
borderline<{BORDERLINE_SCORE}‚Üíshare‚â•{MIN_ECON_SHARE})")
    if cursor != "*":
        print("‚Ü©Ô∏è Resuming from saved cursor")

    fh, writer = init_csv(OUT_PATH)
    try:
         while True:
             params = {
                 "filter": f"x_concepts.id:{ECONOMICS_ID}",
                 "per-page": PER_PAGE_AUTHORS,
                 "cursor": cursor,
                 "select":
"id,display_name,orcid,last_known_institutions,works_count,cited_by_count,x_concepts"
             }
             r = _get(base_url, params=params, timeout=AUTHORS_TIMEOUT)

            # tratamento de 429/5xx/outros e cooldown
            ...

            data = r.json()
            if total_candidates is None:
                total_candidates = data.get("meta", {}).get("count", 0)
                print(f"üî¢ Total available = {total_candidates}")

            results = data.get("results", [])
            if not results:
                break

            kept_this_page = 0
            for a in results:
                ok, det = author_passes_field_filter_strict(a, econ_desc)
                if not ok:
                    continue

                lki = a.get("last_known_institutions") or []
                inst = lki[0] if (isinstance(lki, list) and lki) else {}

                writer.writerow({
                    "author_id": a.get("id"),
                    "name": a.get("display_name"),
                    "orcid": a.get("orcid"),
                    "institution_id": inst.get("id", "N/A"),




                               "affiliation": inst.get("display_name", "N/A"),
                               "country": inst.get("country_code", "N/A"),
                               "works_count": a.get("works_count", 0),
                               "cited_by_count": a.get("cited_by_count", 0),
                               "fields": "; ".join([c.get("display_name", "") for c in
  (a.get("x_concepts") or [])]),
                      "field_group": ECONOMICS_NAME,
                      "primary_concept_id": det.get("primary_concept_id"),
                      "primary_concept_name": det.get("primary_concept_name"),
                               "primary_concept_score": det.get("primary_concept_score"),
                               "best_in_field_score": det.get("best_in_field_score"),
                               "best_in_field_id": det.get("best_in_field_id"),
                               "best_in_field_name": det.get("best_in_field_name"),
                               "is_primary_in_field": det.get("is_primary_in_field"),
                           })
                           kept_this_page += 1
                           kept_total += 1

              scanned += len(results)
              print(f"üìä Scanned {scanned} | kept {kept_total} (+
  {kept_this_page}) | sleep {sleep_s:.2f}s")

                  next_cursor = data.get("meta", {}).get("next_cursor")
                  save_cursor(next_cursor)
                  cursor = next_cursor
                  if not cursor:
                      break

                  if scanned % (PER_PAGE_AUTHORS * 5) == 0:
                      fh.flush()

                  if _SHOULD_STOP:
                      print("üõü Graceful stop ‚Äî checkpoint saved.")
                      break

                  time.sleep(sleep_s)
       finally:
             fh.close()
             print(f"üíæ CSV closed: {OUT_PATH}")
```

##### Destaques:

- **Filtro inicial em**   `/authors`   j√°   restringe   a   candidatos   ligados   a   Economia ( `x_concepts.id:<ECON_ID>` ), reduzindo ru√≠do.
- `select` **enxuto**: s√≥ traz os campos necess√°rios.
- **Tratamento robusto** de 429/5xx e ajuste de `sleep` (backoff/cooldown).
- **Flush peri√≥dico** do arquivo para reduzir risco de perda de dados.
- **Checkpoint** via `cursor` a cada p√°gina.

## 17. Ponto de entrada

```python
if __name__ == "__main__":
    fetch_economics_authors()
```

- Executa a fun√ß√£o principal **somente** quando o arquivo √© rodado diretamente (n√£o em import).

## 18. Boas pr√°ticas embutidas no desenho

- **Efici√™ncia**: sess√£o HTTP, `select` m√≠nimo, pagina√ß√£o por cursor, e memoiza√ß√£o de contagens.
- **Respeito √† API**: implementa√ß√£o de `Retry-After` , backoff, cooldown e pequenas pausas entre chamadas.
- **Robustez**: retomada por cursor, fechamento garantido de arquivo ( `finally` ), parada graciosa.
- **Precis√£o**: combina√ß√£o de crit√©rios (top-K, absoluto, relativo, propor√ß√£o) reduz falsos positivos.

## 19. Ajustes e personaliza√ß√µes comuns

- **Trocar de campo** (ex.: Computa√ß√£o):
  - Atualize `ECONOMICS_ID` e `ECONOMICS_NAME` para o conceito desejado.
  - O restante da l√≥gica (sub√°rvore, filtros) funciona igual.
- **Ajustar rigor** dos filtros:
  - Aumente `MIN_ECON_SCORE` e/ou `MIN_ECON_RELATIVE` para maior seletividade.
  - Reduza `REQUIRE_ECON_TOP_K` se quiser aceitar autores com Economia fora do top-5 (menos estrito).
  - Ajuste `BORDERLINE_SCORE` e `MIN_ECON_SHARE` conforme a toler√¢ncia.
- **Desempenho e limites de taxa**:
  - Ajuste `SLEEP` , `MIN_SLEEP` / `MAX_SLEEP` , `BACKOFF_MULT` , `COOLDOWN_MULT` segundo sua experi√™ncia de uso.
- **Formato de sa√≠da**:
  - Voc√™ pode trocar o CSV por Parquet/JSON facilmente, se preferir (ex.: usando pandas ).

## 20. Erros comuns e como lidar

- **429 Too Many Requests**: o script j√° respeita `Retry-After` e aumenta `sleep` . Se persistir, considere aumentar `SLEEP` e reduzir `PER_PAGE_AUTHORS` .
- **5xx do servidor**: s√£o transit√≥rios; o script faz backoff e tenta novamente.
- **Conex√µes inst√°veis**: ajuste `*_TIMEOUT` (ex.: `WORKS_TIMEOUT=35` ).
- **CSV corrompido** (queda de energia): o script faz `flush` peri√≥dico e fecha no `finally` , reduzindo
  impacto. Se necess√°rio, remova a √∫ltima linha incompleta.
- **Interrup√ß√£o do usu√°rio**: use Ctrl+C; o script finalizar√° a p√°gina atual, salvar√° o cursor e fechar√° o CSV.

#### Encerramento

Com este guia, voc√™ tem a documenta√ß√£o completa do funcionamento do script, incluindo as motiva√ß√µes de cada par√¢metro, os pontos de robustez e caminhos de customiza√ß√£o. Se quiser, posso preparar uma variante para outro campo (ex.: Computer Science ) ou integrar com pandas para an√°lises adicionais depois da coleta.

## Requerimentos para RA ‚Äî pr√≥ximos passos

1. **Otimiza√ß√£o de desempenho** - Revisar pontos de lat√™ncia (pr√©-carregamento, pagina√ß√£o, E/S em disco) e aplicar profiling (ex.: `cProfile` ) para identificar gargalos. - Reduzir chamadas redundantes, agrupar `select` m√≠nimos e calibrar `SLEEP` / `BACKOFF` com m√©tricas reais. - Considerar paralelismo controlado (fila com taxa m√°xima) e cache persistente das contagens mais caras.
2. **Ampliar n√∫mero de fields** - Generalizar o pipeline para m√∫ltiplas √°reas al√©m de Economia: **Medicina, Sociologia, M√∫sica, Finan√ßas, Estat√≠stica, Ci√™ncia Pol√≠tica, Engenharias**, etc. - Parametrizar `FIELD_ID` / `FIELD_NAME` via CLI ou arquivo de configura√ß√£o; executar em lote com checkpoint por √°rea.
3. **Revis√£o da m√©trica de score** - O score atual pode **excluir** especialistas leg√≠timos ou **incluir** generalistas por ru√≠do (um paper fora da √°rea principal). - Propor um modelo h√≠brido: peso relativo + hist√≥rico temporal + diversidade de venues + percentil dentro do field, e *downweight* de outliers ocasionais. - Introduzir limites por **propor√ß√£o temporal** (√∫ltimos N anos) e por **concentra√ß√£o de venues** (journals/confer√™ncias core do field).
4. **Lista de artigos publicados (journals)** - Incluir, quando dispon√≠vel, uma **lista de journal articles** de cada autor. - Integrar com **ORCID** para obter produ√ß√µes validadas pelo pr√≥prio pesquisador; cruzar com OpenAlex para metadados (DOI, journal, ano, cita√ß√µes). - Exportar campo adicional no CSV (ou arquivo separado) com os artigos por autor (DOI, t√≠tulo, ano, venue, URL).
